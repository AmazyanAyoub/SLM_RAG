# Main config (Copy the one I gave you here)
# configs/base.yaml

project:
  name: "SLM_RAG_2025"
  description: "SOTA RAG with Contextual Retrieval, Agentic Loops, and GraphRAG."

llm:
  fast:
    provider: "ollama"
    model_name: "qwen3:4b" # Fast Assistant
    temperature: 0.1 # Lower temp for factual RAG is better
    top_p: 0.9
    max_tokens: 1024
    base_url: "http://localhost:11434"

  smart:
    provider: "ollama"
    model_name: "qwen3:8b" # Senior Expert
    temperature: 0.1
    max_tokens: 2048
    base_url: "http://localhost:11434"

data_pipeline:
  chunking:
    chunk_size: 512
    chunk_overlap: 100 # Increased slightly for safety
  
  # NEW: Phase 1.5 - Contextual Retrieval
  contextual_retrieval:
    enabled: true
    prompt_template: "Give a short summary of the document this chunk belongs to..."

retrieval:
  # Dense Embeddings (Standard Vector Search)
  embedder:
    provider: "huggingface"
    model_name: "bge-m3"
    normalize_embeddings: true

  # Sparse/Late Interaction (ColBERTv2)
  late_interaction:
    enabled: true
    provider: "ragatouille"
    model_name: "colbert-ir/colbertv2.0"
    index_root: "data/indices/colbert" # RAGatouille stores indices on disk

  # GraphRAG (Phase 4)
  graph:
    enabled: true
    store_type: "networkx" # or "neo4j" if you scale up
    community_level: 2 # How deep to summarize

  # Main Vector Store
  vector_store:
    type: "qdrant"
    host: "localhost"
    port: 6333
    collection: "slm_rag_contextual_V3" # Renamed to indicate contextual data
    api_key: "${QDRANT_API_KEY}" # Use env var placeholder

  # Postgres Vector Store
  postgres:
    host: "localhost"
    port: 5432
    dbname: "postgres"
    user: "postgres"

  # NEW: Phase 2 - User Memory
  memory:
    enabled: true
    provider: "mem0"
    collection_name: "user_preferences"
    history_limit: 10

orchestration:
  engine: "langgraph"
  recursion_limit: 10 # Critical for the "Agentic Loop" to not spin forever

server:
  host: "0.0.0.0"
  port: 8000
  log_level: "info"